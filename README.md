# üßä FrozenLake Q-Learning - Reinforcement Learning

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29%2B-orange)](https://gymnasium.farama.org/)
[![Reinforcement Learning](https://img.shields.io/badge/Reinforcement-Learning-green)](https://www.ibm.com/think/topics/reinforcement-learning)

Uma implementa√ß√£o do algoritmo Q-Learning para resolver o ambiente FrozenLake-v1 do Gymnasium, onde um agente deve aprender a atravessar um lago congelado evitando buracos.

## üìã Sobre o Projeto

Este projeto demonstra os fundamentos do **Aprendizado por Refor√ßo** atrav√©s da implementa√ß√£o pr√°tica do algoritmo **Q-Learning**. O agente come√ßa sem conhecimento pr√©vio e aprende atrav√©s de tentativa e erro a navegar pelo ambiente FrozenLake.

**Problema**: Em um grid 4x4, o agente deve sair do ponto inicial (S) e chegar ao objetivo (G), evitando buracos (H) no gelo.

**Desafio**: O ambiente √© "escorregadio" - as a√ß√µes t√™m apenas 33% de chance de sucesso, tornando o aprendizado mais desafiador e realista.

### üó∫Ô∏è Visualiza√ß√£o do Ambiente
SFFF

FHFH

FFFH

HFFG

üü¶ S = Starting point (Safe) - Ponto de partida

‚¨ú F = Frozen surface (Safe) - Superf√≠cie congelada segura

üï≥Ô∏è H = Hole (Game Over) - Buraco (Fim de jogo)

üéØ G = Goal (Success) - Objetivo (Sucesso)

## üß† O Algoritmo: Q-Learning

### Conceitos Fundamentais

- **Q-Table**: Tabela que armazena o valor esperado de cada par estado-a√ß√£o
- **Pol√≠tica Œµ-greedy**: Balanceia explora√ß√£o (a√ß√µes aleat√≥rias) e explora√ß√£o (melhores a√ß√µes conhecidas)
- **Equa√ß√£o de Atualiza√ß√£o**:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥ * max(Q(s',a')) - Q(s,a)]

### Hiperpar√¢metros Utilizados

| Par√¢metro | Valor | Descri√ß√£o |
|-----------|-------|-----------|
| Œ± (alpha) | 0.8 | Taxa de aprendizado |
| Œ≥ (gamma) | 0.95 | Fator de desconto |
| Œµ (epsilon) | 1.0 ‚Üí 0.01 | Taxa de explora√ß√£o (com decaimento) |
| Epis√≥dios | 20,000 | N√∫mero de sess√µes de treinamento |

## üìä Resultados Esperados

Ap√≥s a execu√ß√£o completa do treinamento (cerca de 2-5 minutos), o script exibir√°:
### Desempenho T√≠pico
- **Taxa de sucesso**: 70-85% em ambiente escorregadio
- **Estabilidade**: Pol√≠tica converge ap√≥s ~15,000 epis√≥dios
- **Robustez**: Agente aprende caminhos alternativos devido √† natureza estoc√°stica

## üèóÔ∏è Estrutura do C√≥digo

### Componentes Principais

1. **Inicializa√ß√£o do Ambiente**
   - Cria√ß√£o do ambiente FrozenLake-v1
   - Configura√ß√£o do modo "escorregadio"

2. **Q-Table**
   - Matriz estados √ó a√ß√µes inicializada com zeros
   - Atualizada iterativamente durante o treinamento

3. **Loop de Treinamento**
   - Pol√≠tica Œµ-greedy para sele√ß√£o de a√ß√µes
   - Atualiza√ß√£o da Q-table usando a equa√ß√£o do Q-Learning
   - Decaimento gradual da explora√ß√£o

4. **Avalia√ß√£o**
   - Teste da pol√≠tica aprendida (sem explora√ß√£o)
   - Medi√ß√£o da taxa de sucesso em 1000 epis√≥dios

## üéØ Aprendizados e Insights

### Conceitos Demonstrados
- **Explora√ß√£o vs Explora√ß√£o**: Como balancear descoberta e aproveitamento do conhecimento
- **Aprendizado Temporal-Difference**: Atualiza√ß√µes baseadas em estimativas futuras
- **Pol√≠ticas de Decis√£o**: Desenvolvimento de estrat√©gias √≥timas atrav√©s de refor√ßo

### Desafios Superados
- **Ambiente Estoc√°stico**: A√ß√µes nem sempre produzem o resultado esperado
- **Recompensas Escassas**: Apenas recompensa no objetivo final
- **Converg√™ncia Lenta**: Necessidade de muitas itera√ß√µes para estabiliza√ß√£o
